var documenterSearchIndex = {"docs":
[{"location":"lib/message/#lib-message","page":"Messages","title":"Messages implementation","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"In message passing framework one of the most important concept is (wow!) messages. Messages flow on edges of a factor graph and usually hold some information in a form of probability distribution. In ReactiveMP.jl we distinguish two major types of messages: Belief Propagation and Variational.  ","category":"page"},{"location":"lib/message/#Abstract-message-type","page":"Messages","title":"Abstract message type","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Both belief propagation and variational messages are subtypes of a AbstractMessage supertype.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"AbstractMessage","category":"page"},{"location":"lib/message/#ReactiveMP.AbstractMessage","page":"Messages","title":"ReactiveMP.AbstractMessage","text":"AbstractMessage\n\nAn abstract supertype for all concrete message types.\n\nSee also: Message\n\n\n\n\n\n","category":"type"},{"location":"lib/message/#lib-belief-propagation-message","page":"Messages","title":"Belief-Propagation (or Sum-Product) message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Belief propagation messages are encoded with type Message. ","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Belief propagation message","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Message","category":"page"},{"location":"lib/message/#ReactiveMP.Message","page":"Messages","title":"ReactiveMP.Message","text":"Message{D} <: AbstractMessage\n\nMessage structure encodes a Belief Propagation message, which holds some data that usually a probability distribution, but can also be an arbitrary object. Message acts as a proxy structure to data object and proxies most of the statistical functions, e.g. mean, mode, cov etc.\n\nArguments\n\ndata::D: message always holds some data object associated with it\nis_clamped::Bool, specifies if this message is clamped\nis_initial::Bool, specifies if this message is initial\n\nExample\n\njulia> distribution = Gamma(10.0, 2.0)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> message = Message(distribution, false, true)\nMessage(Gamma{Float64}(α=10.0, θ=2.0))\n\njulia> mean(message) \n20.0\n\njulia> getdata(message)\nGamma{Float64}(α=10.0, θ=2.0)\n\njulia> is_clamped(message)\nfalse\n\njulia> is_initial(message)\ntrue\n\n\nSee also: AbstractMessage, materialize!\n\n\n\n\n\n","category":"type"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"From implementation point a view Message structure does nothing but holds some data object and redirects most of the statistical related functions to that data object. However it used extensively in Julia's multiple dispatch. Implementation also uses extra is_initial and is_clamped fields to determine if product of two messages results in is_initial or is_clamped posterior marginal.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"distribution = NormalMeanPrecision(0.0, 1.0)\nmessage      = Message(distribution, false, true)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"mean(message), precision(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"logpdf(message, 1.0)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"is_clamped(message), is_initial(message)","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"User should not really interact with Message structure while working with ReactiveMP unless doing some advanced inference procedures that involves prediction.","category":"page"},{"location":"lib/message/#lib-variational-message","page":"Messages","title":"Variational message","text":"","category":"section"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"Variational messages are encoded with type VariationalMessage.","category":"page"},{"location":"lib/message/","page":"Messages","title":"Messages","text":"(Image: message) Variational message with structured factorisation q(x, y)q(z) assumption","category":"page"},{"location":"examples/linear_gaussian_state_space_model/#examples-linear-gaussian-state-space-model","page":"Linear Gaussian Dynamical System","title":"Example: Linear Gaussian State Space Model","text":"","category":"section"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"In this example the goal is to estimate hidden states of a Linear Dynamical process where all hidden states are Gaussians. A simple multivariate Linear Gaussian State Space Model can be described with the following equations:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To model this process in ReactiveMP, first, we start with importing all needed packages:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"using Rocket, ReactiveMP, GraphPPL, Distributions\nusing BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300\n\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"note: Note\nFor large number of observations you will need yo use limit_stack_depth = 100 option during model creation, e.g. model, (x, y) = create_model(..., options = (limit_stack_depth = 100, ))","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x, y = generate_data(rng, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Lets plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\npx = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, y |> slicedim(1), label = false, markersize = 2, color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, y |> slicedim(2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@model function rotate_ssm(n, x0, A, B, Q, P)\n    \n    # We create constvar references for better efficiency\n    cA = constvar(A)\n    cB = constvar(B)\n    cQ = constvar(Q)\n    cP = constvar(P)\n    \n    # `x` is a sequence of hidden states\n    x = randomvar(n)\n    # `y` is a sequence of \"clamped\" observations\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(cA * x_prev, cQ)\n        y[i] ~ MvNormalMeanCovariance(cB * x[i], cP)\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"Also for convenience we create an inference function to infer hidden states of our system:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"function inference(data, x0, A, B, Q, P)\n\n    # We create a model and get references for \n    # hidden states and observations\n    model, (x, y) = rotate_ssm(n, x0, A, B, Q, P);\n\n    xbuffer   = buffer(Marginal, n)\n    bfe       = nothing\n    \n    # We subscribe on posterior marginals of `x`\n    xsubscription = subscribe!(getmarginals(x), xbuffer)\n    # We are also intereset in BetheFreeEnergy functional,\n    # which in this case is equal to minus log evidence\n    fsubcription = subscribe!(score(BetheFreeEnergy(), model), (v) -> bfe = v)\n\n    # `update!` updates our clamped datavars\n    update!(y, data)\n\n    # It is important to always unsubscribe\n    unsubscribe!((xsubscription, fsubcription))\n    \n    return xbuffer, bfe\nend","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"To run inference we also specify prior for out first time-step:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2))\nnothing # hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"xmarginals, bfe = inference(y, x0, A, B, Q, P)\nnothing #hide","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"px = plot()\n\npx = plot!(px, x |> slicedim(1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, x |> slicedim(2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, mean.(xmarginals) |> slicedim(1), ribbon = var.(xmarginals) |> slicedim(1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, mean.(xmarginals) |> slicedim(2), ribbon = var.(xmarginals) |> slicedim(2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"bfe","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"We may be also interested in performance of our resulting Belief Propagation algorithm:","category":"page"},{"location":"examples/linear_gaussian_state_space_model/","page":"Linear Gaussian Dynamical System","title":"Linear Gaussian Dynamical System","text":"@benchmark inference($y, $x0, $A, $B, $Q, $P)","category":"page"},{"location":"lib/node/#lib-node","page":"Factor nodes","title":"Nodes implementation","text":"","category":"section"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"In message passing framework one of the most important concept is factor node.  Factor node represents a local function in a factorised representation of a generative model.","category":"page"},{"location":"lib/node/#lib-node-traits","page":"Factor nodes","title":"Node traits","text":"","category":"section"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"Each factor node has to define as_node_functional_form trait function and to specify ValidNodeFunctionalForm singleton as a return object. By default as_node_functional_form returns UndefinedNodeFunctionalForm. Objects that do not specify this property correctly cannot be used in model specification.","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"note: Note\n@node macro does that automatically","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"ValidNodeFunctionalForm\nUndefinedNodeFunctionalForm\nas_node_functional_form","category":"page"},{"location":"lib/node/#ReactiveMP.ValidNodeFunctionalForm","page":"Factor nodes","title":"ReactiveMP.ValidNodeFunctionalForm","text":"ValidNodeFunctionalForm\n\nTrait specification for an object that can be used in model specification as a factor node.\n\nSee also: as_node_functional_form, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.UndefinedNodeFunctionalForm","page":"Factor nodes","title":"ReactiveMP.UndefinedNodeFunctionalForm","text":"UndefinedNodeFunctionalForm\n\nTrait specification for an object that can not be used in model specification as a factor node.\n\nSee also: as_node_functional_form, ValidNodeFunctionalForm\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.as_node_functional_form","page":"Factor nodes","title":"ReactiveMP.as_node_functional_form","text":"as_node_functional_form(object)\n\nDetermines object node functional form trait specification. Returns either ValidNodeFunctionalForm() or UndefinedNodeFunctionalForm().\n\nSee also: ValidNodeFunctionalForm, UndefinedNodeFunctionalForm\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#lib-node-types","page":"Factor nodes","title":"Node types","text":"","category":"section"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"We distinguish different types of factor nodes to have a better control over Bethe Free Energy computation. Each factor node has either Deterministic or Stochastic functional form type.","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"Deterministic\nStochastic\nisdeterministic\nisstochastic\nsdtype","category":"page"},{"location":"lib/node/#ReactiveMP.Deterministic","page":"Factor nodes","title":"ReactiveMP.Deterministic","text":"Deterministic\n\nDeterministic object used to parametrize factor node object with determinstic type of relationship between variables.\n\nSee also: Stochastic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.Stochastic","page":"Factor nodes","title":"ReactiveMP.Stochastic","text":"Stochastic\n\nStochastic object used to parametrize factor node object with stochastic type of relationship between variables.\n\nSee also: Deterministic, isdeterministic, isstochastic, sdtype\n\n\n\n\n\n","category":"type"},{"location":"lib/node/#ReactiveMP.isdeterministic","page":"Factor nodes","title":"ReactiveMP.isdeterministic","text":"isdeterministic(node)\n\nFunction used to check if factor node object is deterministic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isstochastic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.isstochastic","page":"Factor nodes","title":"ReactiveMP.isstochastic","text":"isstochastic(node)\n\nFunction used to check if factor node object is stochastic or not. Returns true or false.\n\nSee also: Deterministic, Stochastic, isdeterministic, sdtype\n\n\n\n\n\n","category":"function"},{"location":"lib/node/#ReactiveMP.sdtype","page":"Factor nodes","title":"ReactiveMP.sdtype","text":"sdtype(object)\n\nReturns either Deterministic or Stochastic for a given object (if defined).\n\nSee also: Deterministic, Stochastic, isdeterministic, isstochastic\n\n\n\n\n\n","category":"function"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"using ReactiveMP","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"For example + node has the Deterministic type:","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"plus_node = make_node(+)\n\nprintln(\"Is `+` node deterministic: \", isdeterministic(plus_node))\nprintln(\"Is `+` node stochastic: \", isstochastic(plus_node))\nnothing #hide","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"On the other hand Bernoulli node has the Stochastic type:","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"bernoulli_node = make_node(Bernoulli)\n\nprintln(\"Is `Bernoulli` node deterministic: \", isdeterministic(bernoulli_node))\nprintln(\"Is `Bernoulli` node stochastic: \", isstochastic(bernoulli_node))","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"To get an actual instance of the type object we use sdtype function:","category":"page"},{"location":"lib/node/","page":"Factor nodes","title":"Factor nodes","text":"println(\"sdtype() of `+` node is \", sdtype(plus_node))\nprintln(\"sdtype() of `Bernoulli` node is \", sdtype(bernoulli_node))\nnothing #hide","category":"page"},{"location":"lib/node/#lib-node-factorisation-constraints","page":"Factor nodes","title":"Node factorisation constraints","text":"","category":"section"},{"location":"man/getting-started/#user-guide-getting-started","page":"Getting Started","title":"Getting started","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing. It supports both exact and variational inference algorithms.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"ReactiveMP package is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"This page provides the necessary information you need to get started with ReactiveMP. We will show the general approach to solving inference problems with ReactiveMP by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"man/getting-started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Install ReactiveMP through the Julia package manager:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"] add ReactiveMP","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"note: Note\nFor best user experience you also need to install GraphPPL, Rocket and Distributions packages.","category":"page"},{"location":"man/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting Started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The ReactiveMP approach to solving inference problems consists of three phases:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Model specification: ReactiveMP uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: ReactiveMP inference API has been designed to be as flexible as possible and it is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use reactive API from Rocket to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"man/getting-started/#Coin-flip-simulation","page":"Getting Started","title":"Coin flip simulation","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"First lets setup our environment by importing all needed packages:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Next, lets define our dataset:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"rng = MersenneTwister(42)\nn = 10\np = 0.75\ndistribution = Bernoulli(p)\n\ndataset = float.(rand(rng, Bernoulli(p), n))","category":"page"},{"location":"man/getting-started/#Model-specification","page":"Getting Started","title":"Model specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"man/getting-started/#Likelihood","page":"Getting Started","title":"Likelihood","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"man/getting-started/#Prior","page":"Getting Started","title":"Prior","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"theta sim Beta(a b)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"man/getting-started/#Joint-probability","page":"Getting Started","title":"Joint probability","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"\n# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during inference step\n    return y, θ\nend\n","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"As you can see, GraphPPL offers a model specification syntax that resembles closely to the mathematical equations defined above. We use datavar function to create \"clamped\" variables that take specific values at a later date. θ ~ Beta(2.0, 7.0) expression creates random variable θ and assigns it as an output of Beta node in the corresponding FFG. ","category":"page"},{"location":"man/getting-started/#Inference-specification","page":"Getting Started","title":"Inference specification","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Once we have defined our model, the next step is to use ReactiveMP API to infer quantities of interests. To do this, we need to specify inference procedure. ReactiveMP API is flexible in terms of inference specification and is compatible both with real-time inference processing and with static datasets. In most of the cases for static datasets, as in our example, it consists of same basic building blocks:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Return variables of interests from model specification\nSubscribe on variables of interests posterior marginal updates\nPass data to the model\nUnsubscribe ","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here is an example of inference procedure:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"function inference(data)\n    n = length(data)\n\n    # `coin_model` function from `@model` macro returns a reference to the model object and \n    # the same output as in `return` statement in the original function specification\n    model, (y, θ) = coin_model(n)\n    \n    # Reference for future posterior marginal \n    mθ = nothing\n\n    # `getmarginal` function returns an observable of future posterior marginal updates\n    # We use `Rocket.jl` API to subscribe on this observable\n    # As soon as posterior marginal update is available we just save it in `mθ`\n    subscription = subscribe!(getmarginal(θ), (m) -> mθ = m)\n    \n    # `update!` function passes data to our data inputs\n    update!(y, data)\n    \n    # It is always a good practice to unsubscribe and to \n    # free computer resources held by the subscription\n    unsubscribe!(subscription)\n    \n    # Here we return our resulting posterior marginal\n    return mθ\nend","category":"page"},{"location":"man/getting-started/#Inference-execution","page":"Getting Started","title":"Inference execution","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"Here after everything is ready we just call our inference function to get a posterior marginal distribution over θ parameter in the model.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated = inference(dataset)","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"using Plots, LaTeXStrings; theme(:default)\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=L\"P\\:(\\theta|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"In our dataset we used 10 coin flips to estimate the bias of a coin. It resulted in a vague posterior distribution, however ReactiveMP scales very well for large models and factor graphs. We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"dataset_100   = float.(rand(rng, Bernoulli(p), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(p), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(p), 10000))\nnothing # hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"θestimated_100   = inference(dataset_100)\nθestimated_1000  = inference(dataset_1000)\nθestimated_10000 = inference(dataset_10000)\nnothing #hide","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\n\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:100})\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:1000})\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000, x), fillalpha = 0.3, fillrange = 0, label = L\"P\\:(\\theta\\:|y_{1:10000})\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"With larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"println(\"mean: \", mean(θestimated_10000))\nprintln(\"std:  \", std(θestimated_10000))\nnothing #hide","category":"page"},{"location":"man/getting-started/#Where-to-go-next?","page":"Getting Started","title":"Where to go next?","text":"","category":"section"},{"location":"man/getting-started/","page":"Getting Started","title":"Getting Started","text":"There are a set of demos available in ReactiveMP repository that demonstrate the more advanced features of the package and also Examples section in the documentation. Alternatively, you can head to the Model specification which provides more detailed information of how to use ReactiveMP and GraphPPL to specify probabilistic models. Inference execution section provides a documentation about ReactiveMP API for running reactive Bayesian inference.","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with ReactiveMP package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nThis section is WIP and more examples will be added over time.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Gaussian Linear Dynamical System: An example of inference procedure for Gaussian Linear Dynamical System with multivariate noisy observations using Belief Propagation (Sum Product) algorithm. Reference: Simo Sarkka, Bayesian Filtering and Smoothing","category":"page"},{"location":"man/model-specification/#user-guide-model-specification","page":"Model Specification","title":"Model Specification","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Probabilistic models incorporate elements of randomness to describe an event or phenomenon by using random variables and probability theory. A probabilistic model can be represented visually by using probabilistic graphical models (PGMs). A factor graph is a type of PGM that is well suited to cast inference tasks in terms of graphical manipulations.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"GraphPPL.jl is a Julia package presenting a model specification language for probabilistic models.","category":"page"},{"location":"man/model-specification/#@model-macro","page":"Model Specification","title":"@model macro","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The ReactiveMP uses GraphPPL library to simplify model specification. It is not necessary but highly recommended to use ReactiveMP in a combination with GraphPPL model specification library. The GraphPPL library exports a single @model macro for model specification. The @model macro accepts two arguments: model options (optionally) and the model specification itself in a form of regular Julia function. ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"For example: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# `@model` macro accepts an array of named options as a first argument and\n# a regular Julia function body as its second argument\n@model [ option1 = ..., option2 = ... ] function model_name(model_arguments...)\n    # model specification goes here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Model options are optional and may be omitted:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(model_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"that is equivalent to ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# Empty options if ommited\n@model [] function model_name(model_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro returns a regular Julia function (in this example model_name(model_arguments...)) that has the same signature and can be executed as usual. It returns a reference to a model object itself and a tuple of a user specified return variables, e.g:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function my_model(model_arguments...)\n    # model specification here\n    # ...\n    return x, y\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"model, (x, y) = my_model(model_arguments...)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also important to note that any model should return something, such as variables or nodes. If a model doesn't return anything then an error will be raised during runtime.  model object might be useful to inspect model's factor graph and/or factor nodes and variables. It is also used in Bethe Free Energy score computation. If not needed it can be ommited with _ placeholder, eg:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"_, (x, y) = my_model(model_arguments...)","category":"page"},{"location":"man/model-specification/#A-full-example-before-diving-in","page":"Model Specification","title":"A full example before diving in","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Before presenting the details of the model specification syntax, we show an example of a simple probabilistic model. Here we create a linear gaussian state space model with latent random variables x and noisy observations y:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model [ options... ] function state_space_model(n_observations, noise_variance)\n\n    x = randomvar(n_observations)\n    y = datavar(Float64, n_observations)\n\n    x[1] ~ NormalMeanVariance(0.0, 100.0)\n\n    for i in 2:n_observations\n       x[i] ~ x[i - 1] + 1.0\n       y[i] ~ NormalMeanVariance(x[i], noise_variance)\n    end\n\n    return x, y\nend","category":"page"},{"location":"man/model-specification/#Model-variables","page":"Model Specification","title":"Model variables","text":"","category":"section"},{"location":"man/model-specification/#Constants","page":"Model Specification","title":"Constants","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Any runtime constant passed to a model as a model argument will be automatically converted to a fixed constant in the graph model. This convertion happens every time when model specification identifies a constant. Sometimes it might be useful to create constants by hand (e.g. to avoid copying large matrices across the model and to avoid extensive memory allocations).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"You can create a constant within a model specification macro with constvar() function. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"c = constvar(1.0)\n\nfor i in 2:n\n    x[i] ~ x[i - 1] + c # Reuse the same reference to a constant 1.0\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Additionally you can specify an extra ::ConstVariable type for some of the model arguments. In this case macro automatically converts them to a single constant using constvar() function. E.g.:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"@model function model_name(nsamples::Int, c::ConstVariable)\n    # ...\n    # no need to call for a constvar() here\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant `c`\n    end\n    # ...\n    return ...\nend","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\n::ConstVariable does not restrict an input type of an argument and does not interfere with multiple dispatch. In this example c can have any type, e.g. Int.","category":"page"},{"location":"man/model-specification/#user-guide-model-specification-data-variables","page":"Model Specification","title":"Data variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is important to have a mechanism to pass data values to the model. You can create data inputs with datavar() function. As a first argument it accepts a type specification and optional dimensionality (as additional arguments or as a tuple).","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y = datavar(Float64) # Creates a single data input with `y` as identificator\ny = datavar(Float64, n) # Returns a vector of  `y_i` data input objects with length `n`\ny = datavar(Float64, n, m) # Returns a matrix of `y_i_j` data input objects with size `(n, m)`\ny = datavar(Float64, (n, m)) # It is also possible to use a tuple for dimensionality, it is an equivalent of the previous line","category":"page"},{"location":"man/model-specification/#Random-variables","page":"Model Specification","title":"Random variables","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"There are several ways to create random variables. The first one is an explicit call to randomvar() function. By default it doesn't accept any argument, creates a single random variable in the model and returns it. It is also possible to pass dimensionality arguments to randomvar() function in the same way as for the datavar() function.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"x = randomvar() # Returns a single random variable which can be used later in the model\nx = randomvar(n) # Returns an vector of random variables with length `n`\nx = randomvar(n, m) # Returns a matrix of random variables with size `(n, m)`\nx = randomvar((n, m)) # It is also possible to use a tuple for dimensionality, it is an equivalent of the previous line","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The second way to create a random variable is to use the ~ operator. If the random variable hasn't been created yet, ~ operator will be creat it automatically during the creation of the node. Read more about the ~ operator in the next section.","category":"page"},{"location":"man/model-specification/#Factor-nodes","page":"Model Specification","title":"Factor nodes","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Factor nodes (or local functions) are used to define a relationship between random variables and/or constants and data inputs. In most of the cases a factor node defines a probability distribution over selected random variables. ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"We model a random variable by a probability distribution using the ~ operator. For example, to create a random variable y which is modeled by a Normal distribution, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"m = randomvar()\nv = randomvar()\ny ~ NormalMeanVariance(m, v) # Creates a `y` random variable automatically","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also possible to use a deterministic relationships between random variables:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"a = randomvar()\nb = randomvar()\nc ~ a + b # Here with the help of `~` operator we explictly say that `c` is a random variable too","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"note: Note\nThe GraphPPL.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"The @model macro automatically resolves any inner function calls into anonymous extra nodes. It is also worth to note that inference backend will try to optimize inner deterministic function calls in the case where all arguments are constants or data inputs. For example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"noise ~ NormalMeanVariance(mean, inv(precision)) # Will create a non-linear node `inv` in case if `precision` is a random variable. Won't create an additional non-linear node in case if `precision` is a constant or data input.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is possible to use any functional expression within the ~ operator arguments list. The only one exception is the ref expression (e.g x[i] or x[i, j]). In principle x[i] expression is equivalent to getindex(x, i) and therefore might be treated as a factor node with getindex as local function, however all ref expressions within the ~ operator arguments list are left untouched during model parsing. This means that the model parser will not create unnecessary nodes when only simple indexing is involved.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(x[i - 1], variance) # While in principle `x[i - 1]` is equivalent to (`getindex(x, -(i, 1))`) model parser will leave it untouched and won't create any anonymous nodes for this expression.\n\ny ~ NormalMeanVariance(A * x[i - 1], variance) # This example will create a `*` anonymous node (in case if x[i - 1] is a random variable) and leave `x[i - 1]` untouched.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"It is also possible to return a node reference from the ~ operator with the following syntax:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"node, y ~ NormalMeanVariance(mean, var)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Having a node reference can be useful in case the user wants to return it from a model and to use it later on to specify initial joint marginal distributions.","category":"page"},{"location":"man/model-specification/#Node-creation-options","page":"Model Specification","title":"Node creation options","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"To pass optional arguments to the node creation constructor the user can use the where { options...  } specification syntax.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) } # mean-field factorisation over q","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A list of all available options is presented below:","category":"page"},{"location":"man/model-specification/#Factorisation-constraint-option","page":"Model Specification","title":"Factorisation constraint option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Users can specify a factorisation constraint over the approximate posterior q for variational inference. The general syntax for factorisation constraints over q is the following:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"variable ~ Node(node_arguments...) where { q = RecognitionFactorisationConstraint }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"where RecognitionFactorisationConstraint can be one the following:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"MeanField()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a mean-field factorisation","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = MeanField() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"FullFactorisation()","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Automatically specifies a full factorisation (this is the default)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Example:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = FullFactorisation() }","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"q(μ)q(v)q(out) or q(μ) * q(v) * q(out)","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"A user can specify any factorisation he wants as the multiplication of q(interface_names...) factors. As interface names the user can use the interface names of an actual node (read node's documentation), its aliases (if available) or actual random variable names present in the ~ operator expression.","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Examples: ","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"# Using interface names of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names for some node\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(v)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ, v)q(out) }\n\n# Using interface names aliases of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names aliases for some node\n# In general aliases correspond to the function names for distribution parameters\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean)q(var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean, var)q(out) }\n\n# Using random variables names from `~` operator expression\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, y_var)q(y) }\n\n# All methods can be combined easily\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(y_var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, v)q(y) }","category":"page"},{"location":"man/model-specification/#Metadata-option","page":"Model Specification","title":"Metadata option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option (if node supports it, read node's documentation). Metadata can be later accessed in message computation rules:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"z ~ f(x, y) where { meta = ... }","category":"page"},{"location":"man/model-specification/#Pipeline-option","page":"Model Specification","title":"Pipeline option","text":"","category":"section"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"To assign a factor node's local pipeline we use a pipeline option:","category":"page"},{"location":"man/model-specification/","page":"Model Specification","title":"Model Specification","text":"y ~ NormalMeanVariance(m, v) where { pipeline = LoggerPipelineStage() } # Logs all outbound messages with `LoggerPipelineStage`","category":"page"},{"location":"lib/math/#lib-math","page":"Math utils","title":"Math utilities","text":"","category":"section"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"ReactiveMP package exports tiny and huge objects to represent tiny and huge numbers. These objects aren't really numbers and behave differently depending on the context. They do support any operation that is defined for Real numbers. For more info see Julia's documentation about promotion.","category":"page"},{"location":"lib/math/","page":"Math utils","title":"Math utils","text":"tiny\nhuge\nTinyNumber\nHugeNumber","category":"page"},{"location":"lib/math/#ReactiveMP.tiny","page":"Math utils","title":"ReactiveMP.tiny","text":"tiny\n\nAn instance of a TinyNumber. Behaviour and actual value of the tiny number depends on the context.\n\nExample\n\njulia> tiny\ntiny\n\njulia> 1 + tiny\n1.000000000001\n\njulia> tiny + 1\n1.000000000001\n\njulia> 1f0 + tiny\n1.000001f0\n\njulia> big\"1.0\" + tiny\n1.000000000000000000000001\n\njulia> big\"1\" + tiny\n1.000000000000000000000001\n\nSee also: huge, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.huge","page":"Math utils","title":"ReactiveMP.huge","text":"huge\n\nAn instance of a HugeNumber. Behaviour and actual value of the huge number depends on the context.\n\nExample\n\njulia> huge\nhuge\n\njulia> 1 + huge\n1.000000000001e12\n\njulia> huge + 1\n1.000000000001e12\n\njulia> 1f0 + huge\n1.000001f6\n\njulia> big\"1.0\" + huge\n1.000000000000000000000001e+24\n\njulia> big\"1\" + huge\n1.000000000000000000000001e+24\n\nSee also: tiny, TinyNumber, HugeNumber\n\n\n\n\n\n","category":"constant"},{"location":"lib/math/#ReactiveMP.TinyNumber","page":"Math utils","title":"ReactiveMP.TinyNumber","text":"TinyNumber <: Real\n\nTinyNumber represents (wow!) tiny number that can be used in a various computations without unnecessary type promotions.\n\nSee also: HugeNumber\n\n\n\n\n\n","category":"type"},{"location":"lib/math/#ReactiveMP.HugeNumber","page":"Math utils","title":"ReactiveMP.HugeNumber","text":"HugeNumber <: Real\n\nHugeNumber represents (wow!) huge number that can be used in a various computations without unnecessary type promotions.\n\nSee also: TinyNumber\n\n\n\n\n\n","category":"type"},{"location":"man/fundamentals/#user-guide-fundamentals","page":"Fundamentals","title":"Fundamentals","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorials covers the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorial also exists in the form of a Jupyter notebook in demo/ folder at GitHub repository.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"First lets setup our environment by importing all needed packages:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-model-syntax","page":"Fundamentals","title":"General model specification syntax","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and to specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the Model specification section for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision)\n    \n    # We use the `randomvar` function to create \n    # a random variable in our model\n    s = randomvar()\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ GaussianMeanPrecision(s_mean, s_precision)\n    \n    # We use the `datavar` function to create \n    # observed data variables in our models\n    # We also need to specify the type of our data \n    # In this example it is `Float64`\n    y = datavar(Float64)\n    \n    y ~ GaussianMeanPrecision(s, 1.0)\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision) in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (s, y) = test_model1(0.0, 1.0)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Later on we can examine our model structure with the help of some utility functions such as: ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getnodes(): returns an array of factor nodes in a correposning factor graph\ngetrandom(): returns an array of random variable in the model\ngetdata(): returns an array of data inputs in the model\ngetconstant(): return an array of constant values in the model","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getnodes(model)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getrandom(model) .|> name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getdata(model) .|> name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getconstant(model) .|> getconst","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model2(n)\n    \n    if n <= 1\n        error(\"`n` argument must be greater than one.\")\n    end\n    \n    # `randomvar(n)` creates a dense sequence of \n    # random variables\n    s = randomvar(n)\n    \n    # `datavar(Float64, n)` creates a dense sequence of \n    # observed data variables of type `Float64`\n    y = datavar(Float64, n)\n    \n    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\n    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\n    \n    for i in 2:n\n        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\n        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (s, y) = test_model2(10)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of factor nodes in generated Factor Graph\ngetnodes(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of random variables\ngetrandom(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of data inputs\ngetdata(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# An amount of constant values\ngetconstant(model) |> length","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use complex expression inside the functional dependency expressions","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# s = randomvar() here is optional\n# `~` creates random variables automatically\ns ~ NormalMeanPrecision(0.0, 1.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"An example model which will throw an error:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function error_model1()\n    s = 1.0\n    s ~ NormalMeanPrecision(0.0, 1.0)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"By default the GraphPPL.jl package creates new references for constants (literals like 0.0 or 1.0) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. GraphPPL.jl will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use constvar() function to create and reuse similar constants in the model specification syntax as","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Creates constant reference in a model with a prespecified value\nc = constvar(0.0)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"An example:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n    \n    s = randomvar(n)\n    \n    y = datavar(Vector{Float64}, n)\n    \n    # Here we create constant references\n    # for constant matrices in our model \n    # to make inference more memory efficient\n    cA = constvar(A)\n    cP = constvar(P)\n    cQ = constvar(Q)\n    \n    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\n    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\n    \n    for i in 2:n\n        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\n        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\n    end\n    \n    return s, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ~ expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function test_model()\n\n    # In this example `ynode` refers to the corresponding \n    # `GaussianMeanVariance` node created in the factor graph\n    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n    \n    return ynode, y\nend","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-inference","page":"Fundamentals","title":"Probabilistic inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more infromation and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"man/fundamentals/#Observables","page":"Fundamentals","title":"Observables","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(1000, 1000)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"0\n1\n2\n3\n...","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"1\n9\n25\n...","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"unsubscribe!(subscription2)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The ReactiveMP.jl package returns posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience ReactiveMP.jl only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model ReactiveMP.jl exports two functions: ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getmarginal(x): for a single random variable x\ngetmarginals(xs): for a dense sequence of random variables sx","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the Bernoulli distribution with unknown bias parameter θ. To have a fully Bayesian treatment of this problem we endow θ with the Beta prior.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function coin_toss_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during the inference step\n    return y, θ\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (y, θ) = coin_toss_model(500)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# As soon as we have a new value for the marginal posterior over the `θ` variable\n# we simply print the first two statistics of it\nθ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Next, lets define our dataset:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"p       = 0.75 # Bias of a coin\ndataset = float.(rand(Bernoulli(p), 500));","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To pass data to our model we use update! function","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"update!(y, dataset)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It is necessary to always unsubscribe from running observables\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\nupdate!(y, dataset)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\nθvalues = keep(Marginal)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θvalues)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θvalues)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n# an array of updates for a sequence of random variables\nθbuffer = buffer(Marginal, 1)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θbuffer)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(θbuffer)","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-reactive-inference","page":"Fundamentals","title":"Reactive inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function online_coin_toss_model()\n    \n    # We create datavars for the prior \n    # over `θ` variable\n    θ_a = datavar(Float64)\n    θ_b = datavar(Float64)\n    \n    θ ~ Beta(θ_a, θ_b)\n    \n    y = datavar(Float64)\n    y ~ Bernoulli(θ)\n\n    return θ_a, θ_b, θ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (θ_a, θ_b, θ, y) = online_coin_toss_model()\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example we subscribe on posterior marginal of θ variable and use it as a prior for our next observation\n# We also print into stdout for convenience\nθ_subscription = subscribe!(getmarginal(θ), (m) -> begin \n    m_a, m_b = params(m)\n    update!(θ_a, m_a)\n    update!(θ_b, m_b)\n    println(\"New posterior for θ: mean = \", mean(m), \", std = \", std(m))\nend)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Initial priors\nupdate!(θ_a, 10.0 * rand())\nupdate!(θ_b, 10.0 * rand())","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"data_source = timer(500, 500) |> map(Float64, (_) -> float(rand(Bernoulli(0.75)))) |> tap((v) -> println(\"New observation: \", v))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"data_subscription = subscribe!(data_source |> take(5), (data) -> update!(y, data))\nsleep(5) #hide\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It is important to unsubscribe from running observables to release computer resources\nunsubscribe!(data_subscription)\nunsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, ReactiveMP.jl is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-vmp-inference","page":"Fundamentals","title":"Variational inference in ReactiveMP.jl","text":"","category":"section"},{"location":"man/fundamentals/#Factorisation-constraints","page":"Fundamentals","title":"Factorisation constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. For this purpose the @model macro supports optional where { ... } clauses for every ~ expression in a model specification.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model6(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example we specified an extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"There are several options to specify the mean-field factorisation constraint. ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to use local structured factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"As an option the @model macro accepts optional arguments for model specification, one of which is default_factorisation that accepts MeanField() as its argument for better convenience","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model [ default_factorisation = MeanField() ] function test_model(...)\n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This will autatically impose a mean field factorization constraint over all marginal distributions in our model.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To run inference in this model we again need to create a synthetic dataset:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"real_mean = -3.0\nreal_prec = 5.0\nn         = 1000\n\ndataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model6(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose ReactiveMP.jl export the setmarginal! function:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(μ_values)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(τ_values)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))\nnothing #hide","category":"page"},{"location":"man/fundamentals/#Form-constraints","page":"Fundamentals","title":"Form constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In order to support form constraints, the randomvar() function also supports a where { ... } clause with some optional arguments. One of these arguments is form_constraint that allows us to specify a form constraint to the random variables in our model. Another one is prod_constraint that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function test_model7(n)\n    τ ~ GammaShapeRate(1.0, 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar() where { form_constraint = PointMassFormConstraint() }\n    μ ~ NormalMeanVariance(0.0, 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example we specified an extra constraints for q_i for Bethe factorisation:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"real_mean = -3.0\nreal_prec = 5.0\nn         = 1000\n\ndataset = rand(Normal(real_mean, inv(sqrt(real_prec))), n)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model7(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, PointMass(1.0))\n\nμ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"getvalues(μ_values)","category":"page"},{"location":"man/fundamentals/#Product-constraints","page":"Fundamentals","title":"Product constraints","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"By default ReactiveMP.jl tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ = randomvar() where { \n    prod_constraint = ProdGeneric(),\n    form_constraint = SampleListFormConstraint() \n}","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. ReactiveMP.jl exports a special prod_constraint called ProdPreserveType especially for that purpose:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }","category":"page"},{"location":"man/fundamentals/#Free-Energy-Computation","page":"Fundamentals","title":"Free Energy Computation","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"During variational inference ReactiveMP.jl optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the score function.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"model, (μ, τ, y) = test_model6(length(dataset))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"bfe_observable   = score(BetheFreeEnergy(), model)\nbfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Reset the model with vague marginals\nsetmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# It always necessary to unsubscribe and release computer resources\nunsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])","category":"page"},{"location":"man/fundamentals/#Meta-data-specification","page":"Fundamentals","title":"Meta data specification","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-custom-nodes-rules","page":"Fundamentals","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"man/fundamentals/#Custom-nodes","page":"Fundamentals","title":"Custom nodes","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To create a custom functional form and to make it available during model specification ReactiveMP.jl exports the @node macro:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"note: Note\nDeterministic nodes do not support factorisation constraints with the where { q = ... } clause.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"man/fundamentals/#Custom-messages-computation-rules","page":"Fundamentals","title":"Custom messages computation rules","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"mu_z = mu_x + mu_y \nV_z = V_x + V_y","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"To specify this in ReactiveMP.jl we use the @node and @rule macros:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"q(z) = int q(z x y) mathrmdxmathrmdy","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"mu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"nu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy ","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"ReactiveMP.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"note: Note\nIn the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"or","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"man/fundamentals/#user-guide-fundamentals-pipeline","page":"Fundamentals","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"In certain situations it might be convenient to customize the default message computational pipeline. GrahpPPL.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Logs all outbound messages\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n# Initialise messages to be vague\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random #hide\n\n@model function coin_toss_model_log(n)\n\n    y = datavar(Float64, n)\n\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\n    \n    return y, θ\nend","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"_, (y, θ) = coin_toss_model_log(5)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"coinflips = float.(rand(Bernoulli(0.5), 5))\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"update!(y, coinflips)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"unsubscribe!(θ_subscription)","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"# Inference is lazy and does not send messages if no one is listening for them\nupdate!(y, coinflips)\nnothing #hide","category":"page"},{"location":"man/fundamentals/","page":"Fundamentals","title":"Fundamentals","text":"This tutorials covered the fundamentals of the ReactiveMP.jl package. For a more advanced usage we refer the interested reader to the other sections of the documentation.","category":"page"},{"location":"extra/contributing/#Contribution-guidelines","page":"Contributing","title":"Contribution guidelines","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome all possible contributors. This page details the some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"extra/contributing/#Reporting-bugs","page":"Contributing","title":"Reporting bugs","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and ReactiveMP for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"extra/contributing/#Suggesting-features","page":"Contributing","title":"Suggesting features","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Does the feature require changes in the core ReactiveMP.jl code? If it doesn't (for example, you would like to add a factor node for a particular application), you can add local extensions in your script/notebook or consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core ReactiveMP.jl code, please open an issue on GitHub and describe your proposal first. This will allow us to discuss your proposal with you before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"extra/contributing/#Contributing-code","page":"Contributing","title":"Contributing code","text":"","category":"section"},{"location":"extra/contributing/#Installing-ReactiveMP","page":"Contributing","title":"Installing ReactiveMP","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We suggest that you use the dev command from the new Julia package manager to install ReactiveMP.jl for development purposes. To work on your fork of ReactiveMP.jl, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] dev git@github.com:your_username/ReactiveMP.jl.git","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The dev command clones ReactiveMP.jl to ~/.julia/dev/ReactiveMP. All local changes to ReactiveMP code will be reflected in imported code.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"extra/contributing/#Committing-code","page":"Contributing","title":"Committing code","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the master branch of the ReactiveMP.jl repository.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Before opening a pull request, please make sure that all tests pass without failing! All demos (can be found in /demo/ directory) and benchmarks (can be found in /benchmark/ directory) have to run without errors as well.","category":"page"},{"location":"extra/contributing/#Style-conventions","page":"Contributing","title":"Style conventions","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"extra/contributing/#Unit-tests","page":"Contributing","title":"Unit tests","text":"","category":"section"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"We use the test-driven development (TDD) methodology for ReactiveMP.jl development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have following filename format: test_*.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"extra/contributing/","page":"Contributing","title":"Contributing","text":"] test ReactiveMP","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution","page":"Inference execution","title":"Inference execution","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"This section explains how to use ReactiveMP reactive API for running inference on probabilistic models that were created with GraphPPL package as explained in Model Specification section.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types):","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) in an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP engine itself isn't aware of different algorithm types and simply does message passing between nodes, however during model specification stage user may specifiy different factorisation constraints around factor nodes by using where { q = ... } syntax. Different factorisation constraints lead to a different message passing update rules.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Inference with ReactiveMP usually consists of the same simple building blocks and designed in such a way to support both static and real-time infinite datasets:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Create a model with @model macro and get a references to random variables and data inputs\nSubscribe to random variable posterior marginal updates \nSubscribe to Bethe Free Energy updates (optional)\nFeed model with observations \nUnsubscribe from posterior marginal updates (optional)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"It is worth to note that Step 5 is optional and in case where observations come from an infinite real-time data stream (e.g. from the internet) it may be justified to never unsubscribe and perform real-time Bayesian inference in a reactive manner as soon as data arrives.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-model-creation","page":"Inference execution","title":"Model creation","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"During model specification stage user decides on variables of interesets in a model and returns them using a return ... statement. As an example consider that we have a simple hierarchical model in which the mean of a Normal distribution is represented by another Normal distribution whose mean is modelled by another Normal distribution.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random\n\n@model function my_model()\n    m2 ~ NormalMeanVariance(0.0, 1.0)\n    m1 ~ NormalMeanVariance(m2, 1.0)\n\n    y = datavar(Float64)\n    y ~ NormalMeanVariance(m1, 1.0)\n\n    # Return variables of interests\n    return m1, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"And later on we may create our model and obtain references for variables of interests:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m1, y) = my_model()\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"On the other hand, if we were interested in the posterior distributions of both m1 and m2 we would then need to return both of them from a model specification, i.e.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function my_model()\n    ...\n    return m1, m2, y\nend\n\nmodel, (m1, m2, y) = my_model()","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model macro also return a reference for a factor graph as its first return value. Factor graph object (named model in previous example) contains all information about all factor nodes in a model as well as random variables and data inputs.","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-marginal-updates","page":"Inference execution","title":"Posterior marginal updates","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package has a reactive API and operates in terms of Observables and Actors. For detailed information about these concepts we refer to Rocket.jl documentation.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We use getmarginal function to get a posterior marginal updates observable:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_updates = getmarginal(m1)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"After that we can subscribe on new updates and perform some actions based on new values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m1_posterior_subscription = subscribe!(m1_posterior_updates, (new_posterior) -> begin\n    println(\"New posterior for m1: \", new_posterior)\nend)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Sometimes it is usefull to return an array of random variables from model specification, in this case we may use getmarginals() function that transform an array of observables to an observable of arrays.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function my_model()\n    ...\n    m_n = randomvar(n)\n    ...\n    return m_n, ...\nend\n\nmodel, (m_n, ...) = my_model()\n\nm_n_updates = getmarginals(m_n)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-execution-observations","page":"Inference execution","title":"Feeding observations","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"By default (without any extra factorisation constraints) model specification implies Belief Propagation message passing update rules. In case of BP algorithm ReactiveMP package computes an exact Bayesian posteriors with a single message passing iteration. To enforce Belief Propagation message passing update rule for some specific factor node user may use where { q = FullFactorisation() } option. Read more in Model Specification section. To perform a message passing iteration we need to pass some data to all our data inputs that were created with datavar function during model specification.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To feed an observation for a specific data input we use update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As you can see after we passed a single value to our data input we got a posterior marginal update from our subscription and printed it with println function. In case of BP if observations do not change it should not affect posterior marginal results:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"update!(y, 0.0) # Observation didn't change, should result in the same posterior\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"If y is an array of data inputs it is possible to pass an array of observation to update! function:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"for i in 1:length(data)\n    update!(y[i], data[i])\nend\n# is an equivalent of\nupdate!(y, data)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp","page":"Inference execution","title":"Variational Message Passing","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In ReactiveMP, the specification of such factorization properties is defined during model specification stage using the where { q = ... } syntax. Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and precision of a Normal distribution, where the former is modelled with a Normal distribution and the latter with a Gamma.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Rocket, GraphPPL, ReactiveMP, Distributions, Random","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"real_mean      = -4.0\nreal_precision = 0.2\nrng            = MersenneTwister(1234)\n\nn    = 100\ndata = rand(rng, Normal(real_mean, sqrt(inv(real_precision))), n)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"@model function normal_estimation(n)\n    m ~ NormalMeanVariance(0.0, 10.0)\n    w ~ Gamma(0.1, 10.0)\n\n    y = datavar(Float64, n)\n\n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w) where { q = MeanField() }\n    end\n\n    return m, w, y\nend","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"We create our model as usual, however in order to start VMP inference procedure we need to set initial posterior marginals for all random variables in the model:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"model, (m, w, y) = normal_estimation(n)\n\n# We use vague initial marginals\nsetmarginal!(m, vague(NormalMeanVariance)) \nsetmarginal!(w, vague(Gamma))\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"To perform a single VMP iteration it is enough to feed all data inputs with some values. To perform multiple VMP iterations we should feed our all data inputs with the same values multiple times:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"m_marginals = []\nw_marginals = []\n\nsubscriptions = subscribe!([\n    (getmarginal(m), (marginal) -> push!(m_marginals, marginal)),\n    (getmarginal(w), (marginal) -> push!(w_marginals, marginal)),\n])\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(subscriptions)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"As we process more VMP iterations, our beliefs about the possible values of m and w converge and become more confident.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"using Plots\n\np1    = plot(title = \"'Mean' posterior marginals\")\ngrid1 = -6.0:0.01:4.0\n\nfor iter in [ 1, 2, 10 ]\n\n    estimated = Normal(mean(m_marginals[iter]), std(m_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p1, grid1, e_pdf, fill = true, opacity = 0.3, label = \"Estimated mean after $iter VMP iterations\")\n\nend\n\nplot!(p1, [ real_mean ], seriestype = :vline, label = \"Real mean\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"p2    = plot(title = \"'Precision' posterior marginals\")\ngrid2 = 0.01:0.001:0.35\n\nfor iter in [ 2, 3, 10 ]\n\n    estimated = Gamma(shape(w_marginals[iter]), scale(w_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p2, grid2, e_pdf, fill = true, opacity = 0.3, label = \"Estimated precision after $iter VMP iterations\")\n\nend\n\nplot!(p2, [ real_precision ], seriestype = :vline, label = \"Real precision\", color = :red4, opacity = 0.7)","category":"page"},{"location":"man/inference-execution/#user-guide-inference-vmp-bfe","page":"Inference execution","title":"Computing Bethe Free Energy","text":"","category":"section"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as Variational Free Energy. ReactiveMP uses Bethe Free Energy approximation to the real Variational Free Energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"The ReactiveMP package exports score function for an observable of free energy values:","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"fe_observable = score(BetheFreeEnergy(), model)\nnothing #hide","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"# Reset posterior marginals for `m` and `w`\nsetmarginal!(m, vague(NormalMeanVariance))\nsetmarginal!(w, vague(Gamma))\n\nfe_values = []\n\nfe_subscription = subscribe!(fe_observable, (v) -> push!(fe_values, v))\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(fe_subscription)","category":"page"},{"location":"man/inference-execution/","page":"Inference execution","title":"Inference execution","text":"plot(fe_values, label = \"Bethe Free Energy\", xlabel = \"Iteration #\")","category":"page"},{"location":"#ReactiveMP.jl","page":"Introduction","title":"ReactiveMP.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Given a probabilistic model, ReactiveMP allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model.","category":"page"},{"location":"#Package-Features","page":"Introduction","title":"Package Features","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nSupport for hybrid models combining discrete and continuous latent variables.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nEvaluation of Bethe free energy as a model performance measure.\nSchedule-free reactive message passing API.\nHigh performance.\nScalability for large models with millions of parameters and observations.\nInference procedure is differentiable.\nEasy to extend with custom nodes.","category":"page"},{"location":"#Resources","page":"Introduction","title":"Resources","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For an introduction to message passing and FFGs, see The Factor Graph Approach to Model-Based Signal Processing by Loeliger et al. (2007).","category":"page"},{"location":"#How-to-get-started?","page":"Introduction","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Head to the Getting started section to get up and running with ForneyLab. Alternatively, explore various examples in the documentation.","category":"page"},{"location":"#Table-of-Contents","page":"Introduction","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Pages = [\n  \"man/getting-started.md\",\n  \"man/fundamentals.md\",\n  \"man/model-specification.md\",\n  \"examples/overview.md\",\n  \"lib/message.md\",\n  \"lib/node.md\",\n  \"lib/math.md\",\n  \"extra/contributing.md\"\n]\nDepth = 2","category":"page"},{"location":"#Index","page":"Introduction","title":"Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"lib/helpers/#lib-helpers","page":"Helper utils","title":"Helper utilities","text":"","category":"section"},{"location":"lib/helpers/#lib-one-div-n-vector","page":"Helper utils","title":"OneDivNVector","text":"","category":"section"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"Helper utilities implement OneDivNVector structure that is allocation free equivalent of fill(1 / N, N) collection. Mostly used in SampleList implementation.","category":"page"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"DocTestSetup = quote\n    using ReactiveMP\n    import ReactiveMP: OneDivNVector\nend","category":"page"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"ReactiveMP.OneDivNVector","category":"page"},{"location":"lib/helpers/#ReactiveMP.OneDivNVector","page":"Helper utils","title":"ReactiveMP.OneDivNVector","text":"OneDivNVector(N::Int)\nOneDivNVector(::Type{T}, N::Int) where T\n\nAllocation-free version of fill(one(T) / N, N) vector.\n\nArguments\n\n::Type{T}: type of elements, optional, Float64 by default, should be a subtype of Number\nN::Int: number of elements in a container, should be greater than zero\n\nExamples\n\njulia> iter = ReactiveMP.OneDivNVector(3)\n3-element ReactiveMP.OneDivNVector{3, Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\njulia> length(iter)\n3\n\njulia> eltype(iter)\nFloat64\n\njulia> collect(iter)\n3-element Vector{Float64}:\n 0.3333333333333333\n 0.3333333333333333\n 0.3333333333333333\n\njulia> iter = ReactiveMP.OneDivNVector(Float32, 3)\n3-element ReactiveMP.OneDivNVector{3, Float32}:\n 0.33333334\n 0.33333334\n 0.33333334\n\njulia> collect(iter)\n3-element Vector{Float32}:\n 0.33333334\n 0.33333334\n 0.33333334\n\nSee also: SampleList\n\n\n\n\n\n","category":"type"},{"location":"lib/helpers/","page":"Helper utils","title":"Helper utils","text":"DocTestSetup = nothing","category":"page"}]
}
